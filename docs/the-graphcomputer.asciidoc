[[graphcomputer]]
The GraphComputer
=================

image:graphcomputer-puffers.png[width=350,float=right] TinkerPop3 provides two primary means of interacting with a graph: link:http://en.wikipedia.org/wiki/Online_transaction_processing[online transaction processing] (OLTP) and link:http://en.wikipedia.org/wiki/Online_analytical_processing[online analytical processing] (OLAP). OTLP-based graph systems allow the user to query the graph in real-time. However, typically, real-time performance is only possible when a local traversal is enacted. A local traversal is one that starts at a particular vertex (or small set of vertices) and touches a small set of connected vertices (by any arbitrary path of arbitrary length). In short, OLTP queries interact with a limited set of data and respond on the order of milliseconds or seconds. On the other hand, with OLAP graph processing, the entire graph is processed and thus, every vertex and edge is analyzed (some times more than once for iterative, recursive algorithms). Due to the amount of data being processed, the results are typically not returned in real-time and for massive graphs (i.e. graphs represented across a cluster of machines), results can take on the order of minutes or hours.

 * *OLTP*: real-time, limited data accessed, random data access, sequential processing, querying
 * *OLAP*: long running, entire data set accessed, sequential data access, parallel processing, batch processing

image::oltp-vs-olap.png[width=600]

The image above demonstrates the difference between Gremlin OLTP and Gremlin OLAP. With Gremlin OLTP, the graph is walked by moving from vertex-to-vertex via incident edges. With Gremlin OLAP, all vertices are provided a `VertexProgram`. The programs send messages to one another with the topological structure of the graph acting as the communication network (though random message passing possible). In many respects, the messages passed are like the OLTP traversers moving from vertex-to-vertex. However, all messages are moving independent of one another, in parallel. Once a vertex program is finished computing, TinkerPop3's OLAP engine supports any number link:http://en.wikipedia.org/wiki/MapReduce[`MapReduce`] jobs over the resultant graph.

IMPORTANT: `GraphComputer` was designed from the start to be used within a multi-JVM, distributed environment -- in other words, a multi-machine compute cluster. As such, all the computing objects must be able to be migrated between JVMs. The pattern promoted is to store state information in a `Configuration` object to later be regenerated by a loading process. It is important to realize that `VertexProgram`, `MapReduce`, and numerous particular instances rely heavily on the state of the computing classes (not the structure, but the processes) to be stored in a `Configuration`.

[[vertexprogram]]
VertexProgram
-------------

image:bsp-diagram.png[width=400,float=right] GraphComputer takes a `VertexProgram`. A VertexProgram can be thought of as a piece of code that is executed at each vertex in logically parallel manner until some termination condition is met (e.g. a number of iterations have occurred, no more data is changing in the graph, etc.). A submitted VertexProgram is copied to all the workers in the graph. A worker is not an explicit concept in the API, but is assumed of all GraphComputer implementations. At minimum each vertex is a worker (though this would be inefficient due to the fact that each vertex would maintain a VertexProgram). In practice, the workers partition the vertex set and and are responsible for the execution of the VertexProgram over all the vertices within their sphere of influence. The workers orchestrate the execution of the `VertexProgram.execute()` method on all their vertices in an link:http://en.wikipedia.org/wiki/Bulk_synchronous_parallel[bulk synchronous parallel] (BSP) fashion. The vertices are able to communicate with one another via messages. There are two types of messages in Gremlin OLAP: `MessageType.Local` and `MessageType.Global`. A local message is a message to an adjacent vertex. A global message is a message to any arbitrary vertex in the graph. Once the VertexProgram has completed its execution, any number of `MapReduce` jobs are evaluated. MapReduce jobs are provided by the user via `GraphComputer.mapReduce()` or by the VertexProgram via `VertexProgram.getMapReducers()`.

image::graphcomputer.png[width=500]

The example below demonstrates how to submit a VertexProgram to a graph's GraphComputer. `GraphComputer.submit()` yields a `Future<ComputerResult>`. The `ComputerResult` has the resultant computed graph which can be a full copy of the original graph (see <<giraph-gremlin,Giraph-Gremlin>>) or a view over the original graph (see <<tinkergraph,TinkerGraph>>). The ComputerResult also provides access to computational side-effects called `Memory` (which includes, for example, runtime, number of iterations, results of MapReduce jobs, and VertexProgram-specific memory manipulations).

[source,groovy]
gremlin> g = TinkerFactory.createClassic()
==>tinkergraph[vertices:6 edges:6]
gremlin> result = g.compute().program(PageRankVertexProgram.build().create()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:0]]
gremlin> result.graph().V().map{[it.get().value('name'), it.get().value(PageRankVertexProgram.PAGE_RANK)]}
==>[marko, 0.15000000000000002]
==>[vadas, 0.19250000000000003]
==>[lop, 0.4018125]
==>[josh, 0.19250000000000003]
==>[ripple, 0.23181250000000003]
==>[peter, 0.15000000000000002]
gremlin> result.memory().runtime
==>35

NOTE: This model of "vertex-centric graph computing" was made popular by Google's link:http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html[Pregel] graph engine. In the open source world, this model is found in OLAP graph computing systems such as link:https://giraph.apache.org/[Giraph], link:https://hama.apache.org/[Hama], and link:http://faunus.thinkaurelius.com[Faunus]. TinkerPop3 extends the popularized model with integrated post-processing <<mapreduce,MapReduce>> jobs over the vertex set.

IMPORTANT: As of TinkerPop3 x.y.z, message combiners are not supported. The primary reason is that TinkerPop wants to provide a model of message combining that does not require all messages to a particular vertex to be combined. This allows for more complex message passing scenarios to exist, where multi-typed messages are possible. However, at this time, no general solution has been developed.

[[mapreduce]]
MapReduce
---------

The BSP model proposed by Pregel stores the results of the computation in a distributed manner as properties on the elements in the graph. In many situations, it is necessary to aggregate those resultant properties into a single result set (i.e. a statistic). For instance, assume a VertexProgram that computes a nominal cluster for each vertex (i.e. link:http://en.wikipedia.org/wiki/Community_structure[a graph clustering algorithm]). At the end of the computation, each vertex will have a property denoting the cluster it was assigned to. TinkerPop3 provides the ability to answer global questions about the clusters. For instance, in order to answer the following questions, `MapReduce` jobs are required:

 * How many vertices are in each cluster? (*presented below*)
 * How many unique clusters are there? (*presented below*)
 * What is the average age of each vertex in each cluster?
 * What is the degree distribution of the vertices in each cluster?

A compressed representation of the `MapReduce` API in TinkerPop3 is provided below. The key idea is that the `map`-stage processes all vertices to emit key/value pairs. Those values are aggregated on their respective key for the `reduce`-stage to do its processing to ultimately yield more key/value pairs.

[source,java]
public interface MapReduce<MK, MV, RK, RV, R> {
  public void map(final Vertex vertex, final MapEmitter<MK, MV> emitter);
  public void reduce(final MK key, final Iterator<MV> values, final ReduceEmitter<RK, RV> emitter);
  // there are more methods
}

image::mapreduce.png[width=650]

The `MapReduce` extension to GraphComputer is made explicit when examining the <<peerpressurevertexprogram,`PeerPressureVertexProgram`>> and corresponding `ClusterPopulationMapReduce`. In the code below, the GraphComputer result returns the computed on `Graph` as well as the `Memory` of the computation (`ComputerResult`). The memory maintain the results of any MapReduce jobs. The cluster population MapReduce result states that there are 5 vertices in cluster 1 and 1 vertex in cluster 6. This can be verified (in a serial manner) by looking at the `PeerPressureVertexProgram.CLUSTER` property of the resultant graph. In essence, the serial process of the final Gremlin traversal is done in a parallel MapReduce fashion using `ClusterPopulationMapReduce`.

[source,groovy]
gremlin> g = TinkerFactory.createClassic()
==>tinkergraph[vertices:6 edges:6]
gremlin> result = g.compute().program(PeerPressureVertexProgram.build().create()).mapReduce(new ClusterPopulationMapReduce()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:2]]
gremlin> result.memory().get('clusterPopulation')
==>1=5
==>6=1
gremlin> result.graph().V().values(PeerPressureVertexProgram.CLUSTER).groupCount().next()
==>1=5
==>6=1

If there are numerous statistics desired, then its possible to register as many MapReduce jobs as needed. For instance, the `ClusterCountMapReduce` determines how many unique clusters were created by the peer pressure algorithm. Below both `ClusterCountMapReduce` and `ClusterPopulationMapReduce` are computed over the resultant graph.

[source,groovy]
gremlin> g = TinkerFactory.createClassic()
==>tinkergraph[vertices:6 edges:6]
gremlin> result = g.compute().program(PeerPressureVertexProgram.build().create()).
 mapReduce(new ClusterPopulationMapReduce()).
 mapReduce(new ClusterCountMapReduce()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:3]]
gremlin> result.memory().clusterPopulation
==>1=5
==>6=1
gremlin> result.memory().clusterCount
==>2

IMPORTANT: The MapReduce model of TinkerPop3 does not support MapReduce chaining. Thus, the order in which the MapReduce jobs are executed is irrelevant. This is made apparent when realizing that the `map()`-stage takes a `Vertex` as its input and the `reduce()`-stage yields key/value pairs. Thus, the results of reduce can not feed back into map.

A Collection of VertexPrograms
------------------------------

TinkerPop3 provides a collection of VertexPrograms that implement common algorithms. This section discusses the various implementations.

IMPORTANT: The vertex programs presented are what are provided as of TinkerPop x.y.z. Over time, with future releases, more algorithms will be added.

[[lambdavertexprogram]]
LambdaVertexProgram
~~~~~~~~~~~~~~~~~~~

image:lambda-vertex-program.png[width=200,float=left] `LambdaVertexProgram` is the most generic of all vertex programs as it requires the user to define, by way of lambdas, the meaning of `setup`, `execute`, and `terminate`. This vertex program is convenient for:

* Creating "one line" vertex programs
* Submitting a "one off" vertex program without having to build a class and distribute jars
* Testing for vendors of `GraphComputer` implementations

[source,groovy]
gremlin> result = g.compute().program(LambdaVertexProgram.build().
                      execute("a.singleProperty('counter', c.isInitialIteration() ? 0 : ++a.value('counter'))").
                      terminate('a.iteration > 9').
                      elementComputeKeys('counter').create()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:0]]
gremlin> result.graph().V().values('counter')
==>10
==>10
==>10
==>10
==>10
==>10

NOTE: If a single string is provided to the LambdaVertexProgram's builder methods, then it is assumed to be a Gremlin-Groovy script. It is possible to use other Gremlin `ScriptEngine` implementations (e.g. Gremlin-Scala, Gremlin-JavaScript, etc.) by ensuring 1.) the script engine is registered in the `META-INF/services` of the application and 2.) it is declared as such `build().execute('gremlin-scala','a.singlePropertyâ€¦')`.

The same example is presented below in Java8 using native lambda syntax.

[source,java]
ComputerResult results = g.compute().program(LambdaVertexProgram.build().
                        execute((vertex, messenger, memory) -> vertex.<Integer>singleProperty("counter", memory.isInitialIteration() ? 0 : vertex.<Integer>value("counter") + 1)).
                        terminate(memory -> memory.getIteration() > 9).
                        elementComputeKeys("counter").create()).submit().get();
results.graph().V().values("counter").forEach(System.out::println);
// 10
// 10
// 10
// 10
// 10
// 10

WARNING: Java8 lambdas are not serializable unless they are static classes. As such, for the last example to execute on a multi-machine GraphComputer a `Class` can be provided denoting the static lambda classes: `build().execute(MyCounterTriConsumer.class)`. It is important that that class be on the class path of all machines in the GraphComputer cluster.

Finally, there also exists `LambdaMapReduce` to compliment `LambdaVertexProgram`. In essence, the `map`, `combine`, `reduce`, etc. methods of <<mapreduce,MapReduce>> can be described by lambdas. An example is provided below in Gremlin-Groovy that expands on the example previous that simply sums up all the counters on the vertices and stores them into a graph computer memory called `counter`.

[source,groovy]
gremlin> result = g.compute().
                      program(LambdaVertexProgram.build().
                        execute("a.singleProperty('counter', c.isInitialIteration() ? 0 : ++a.value('counter'))").
                        terminate('a.iteration > 9').
                        elementComputeKeys('counter').create()).
                      mapReduce(LambdaMapReduce.build().
                        map("b.emit(a.value('counter'))").
                        reduce("c.emit(a,b.sum())").
                        memory('a.next().value1').
                        memoryKey('sum').create()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:1]]
gremlin> result.memory().sum
==>60

TIP: In the last example, the `map()` emits a single value with no key. `MapEmitter.emit(value)` is a default method that assumes the key is the `MapReduce.NullObject` singleton. This method is useful when all values need to be aggregated to a single key -- e.g., when a global sum is needed.

NOTE: The examples presented are simple. For more complex uses within Gremlin Console, it may be important to either develop a `VertexProgram` class or make use of Groovy link:http://groovy.codehaus.org/Strings+and+GString[multi-line strings].

[[pagerankvertexprogram]]
PageRankVertexProgram
~~~~~~~~~~~~~~~~~~~~~

image:gremlin-pagerank.png[width=400,float=right] link:http://en.wikipedia.org/wiki/PageRank[PageRank] is perhaps the most popular OLAP-oriented graph algorithm. This link:http://en.wikipedia.org/wiki/Centrality[eigenvector centrality] variant was developed by Brin and Page of Google. PageRank defines a centrality value for all vertices in the graph, where centrality is defined recursively where a vertex is central if it is connected to central vertices. PageRank is an iterative algorithm that converges to a link:http://en.wikipedia.org/wiki/Ergodicity[steady state distribution]. If the pageRank values are normalized to 1.0, then the pageRank value of a vertex is the probability that a random walker will be seen that that vertex in the graph at any arbitrary moment in time. In order to help developers understand the methods of a `VertexProgram`, the PageRankVertexProgram code is analyzed below.

[source,java]
----
public class PageRankVertexProgram implements VertexProgram<Double> { <1>

    private MessageType.Local<?, ?> messageType = MessageType.Local.of(new OutETraversalSupplier()); <2>

    public static final String PAGE_RANK = Graph.Key.hide("gremlin.pageRankVertexProgram.pageRank"); <3>
    public static final String EDGE_COUNT = Graph.Key.hide("gremlin.pageRankVertexProgram.edgeCount");

    private static final String VERTEX_COUNT = "gremlin.pageRankVertexProgram.vertexCount";
    private static final String ALPHA = "gremlin.pageRankVertexProgram.alpha";
    private static final String TOTAL_ITERATIONS = "gremlin.pageRankVertexProgram.totalIterations";
    private static final String INCIDENT_TRAVERSAL_SUPPLIER = "gremlin.pageRankVertexProgram.incidentTraversalSupplier";

    private LambdaHolder<Supplier<CountTraversal<Vertex, Edge>>> traversalSupplier;
    private double vertexCountAsDouble = 1;
    private double alpha = 0.85d;
    private int totalIterations = 30;

    private static final Set<String> COMPUTE_KEYS = new HashSet<>(Arrays.asList(PAGE_RANK, EDGE_COUNT));

    private PageRankVertexProgram() { }

    @Override
    public void loadState(final Configuration configuration) { <4>
        this.traversalSupplier = LambdaHolder.loadState(configuration, INCIDENT_TRAVERSAL_SUPPLIER);
        if (null != this.traversalSupplier) {
            VertexProgramHelper.verifyReversibility(this.traversalSupplier.get().get());
            this.messageType = MessageType.Local.of(this.traversalSupplier.get());
        }
        this.vertexCountAsDouble = configuration.getDouble(VERTEX_COUNT, 1.0d);
        this.alpha = configuration.getDouble(ALPHA, 0.85d);
        this.totalIterations = configuration.getInt(TOTAL_ITERATIONS, 30);
    }

    @Override
    public void storeState(final Configuration configuration) {
        configuration.setProperty(VERTEX_PROGRAM, PageRankVertexProgram.class.getName());
        configuration.setProperty(VERTEX_COUNT, this.vertexCountAsDouble);
        configuration.setProperty(ALPHA, this.alpha);
        configuration.setProperty(TOTAL_ITERATIONS, this.totalIterations);
        if (null != this.traversalSupplier) {
            this.traversalSupplier.storeState(configuration);
        }
    }

    @Override
    public Set<String> getElementComputeKeys() { <5>
        return COMPUTE_KEYS;
    }

    @Override
    public void setup(final Memory memory) { }

    @Override
    public void execute(final Vertex vertex, Messenger<Double> messenger, final Memory memory) { <6>
        if (memory.isInitialIteration()) { <7>
            double initialPageRank = 1.0d / this.vertexCountAsDouble;
            double edgeCount = Double.valueOf(this.messageType.<CountTraversal<Vertex, Edge>>edges(vertex).count().next());
            vertex.singleProperty(PAGE_RANK, initialPageRank);
            vertex.singleProperty(EDGE_COUNT, edgeCount);
            messenger.sendMessage(this.messageType, initialPageRank / edgeCount);
        } else { <8>
            double newPageRank = StreamFactory.stream(messenger.receiveMessages(this.messageType)).reduce(0.0d, (a, b) -> a + b);
            newPageRank = (this.alpha * newPageRank) + ((1.0d - this.alpha) / this.vertexCountAsDouble);
            vertex.singleProperty(PAGE_RANK, newPageRank);
            messenger.sendMessage(this.messageType, newPageRank / vertex.<Double>property(EDGE_COUNT).orElse(0.0d));
        }
    }

    @Override <9>
    public boolean terminate(final Memory memory) {
        return memory.getIteration() >= this.totalIterations;
    }

    ...

    public static class OutETraversalSupplier implements Supplier<CountTraversal<Vertex, Edge>> {
        public CountTraversal<Vertex, Edge> get() {
            return GraphTraversal.<Vertex>of().outE();
        }
    }
}
----

<1> `PageRankVertexProgram` implements `VertexProgram<Double>` because the messages it sends are Java doubles.
<2> The default path of energy propagation is via outgoing edges from the current vertex.
<3> The resulting PageRank values for the vertices are stored as a hidden property.
<4> A vertex program is constructed using an Apache `Configuration` to ensure easy dissemination across a cluster of JVMs.
<5> A vertex program must define the "compute keys" that are the properties being operated on during the computation.
<6> The "while"-loop of the vertex program.
<7> Initially, each vertex is provided an equal amount of energy represented as a double.
<8> Energy is aggregated, computed on according to the PageRank algorithm, and then disseminated according to the defined `MessageType.Local`.
<9> The computation is terminated after a pre-defined number of iterations.

[[peerpressurevertexprogram]]
PeerPressureVertexProgram
~~~~~~~~~~~~~~~~~~~~~~~~~

The `PeerPressureVertexProgram` is a clustering algorithm that assigns a nominal value to each vertex in the graph. The nominal value represents the vertex's cluster. If two vertices have the same nominal value, then they are in the same cluster. The algorithm proceeds in the following manner.

 . Every vertex assigns itself to a unique cluster ID (initially, its vertex ID).
 . Every vertex determines its per neighbor vote strength as 1.0d / incident edges count.
 . Every vertex sends its cluster ID and vote strength to its adjacent vertices as a `Pair<Serializable,Double>`
 . Every vertex generates a vote energy distribution of received cluster IDs and changes its current cluster ID to the most frequent cluster ID.
  .. If there is a tie, then the cluster with the lowest `toString()` comparison is selected.
 . Steps 3 and 4 repeat until either a max number of iterations has occurred or no vertex has adjusted its cluster anymore.

[[traversalvertexprogram]]
TraversalVertexProgram
~~~~~~~~~~~~~~~~~~~~~~

image:traversal-vertex-program.png[width=250,float=left] The `TraversalVertexProgram` is a "special" VertexProgram in that it can be executed via `GraphTraversal.submit()` and the submit `:>` command in <<gremlin-console,Gremlin Console>>. In Gremlin, it is possible to have the same traversal executed using either the standard OTLP-engine or the `GraphComputer` OLAP-engine. The difference being where the traversal is submitted.

NOTE: This model of graph traversal in a BSP system was first implemented by the link:http://faunus.thinkaurelius.com[Faunus] graph analytics engine and originally described in link:http://markorodriguez.com/2011/04/19/local-and-distributed-traversal-engines/[Local and Distributed Traversal Engines].

[source,groovy]
gremlin> g = TinkerFactory.createClassic()
==>tinkergraph[vertices:6 edges:6]
gremlin> g.V().both().has('age').values('age').groupCount().next() // OLTP
==>32=3
==>35=1
==>27=1
==>29=3
gremlin> g.V().both().has('age').values('age').groupCount().submit(g.compute()).next() // OLAP
==>32=3
==>35=1
==>27=1
==>29=3

In the OLAP example above, a `TraversalVertexProgram` is (logically) sent to each vertex in the graph. Each instance evaluation requires (logically) 5 BSP iterations and each iteration is interpreted as such:

 . `g.V()`: Put a traverser on each vertex in the graph.
 . `both()`: Propagate each traverser to the vertices `both`-adjacent to its current vertex.
 . `has('age')`: If the vertex does not have an `age` property, kill the traversers at that vertex.
 . `values('age')`: Have all the traversers reference the integer age of their current vertex.
 . `groupCount()`: Count how many times a particular age has been seen.

While 5 iterations were presented, in fact, `TraversalVertexProgram` will execute the traversal in only 3 iterations. The reason being is that `has('age').values('age').groupCount()` can all be executed in a single iteration as any message sent would simply be to the current executing vertex. Thus, a simple optimization exists in Gremlin OLAP called "reflexive message passing" which simulates non-message-passing BSP iterations within a single BSP iteration.

When the computation is complete a <<mapreduce,MapReduce>> job executes which aggregates all the `groupCount()` sideEffect Map (i.e. "`HashMap`") objects on each vertex into a single local representation (thus, turning the distributed Map representation into a local Map representation).

The same OLAP traversal can be executed using the standard `g.compute()` model, though at the expense of verbosity. `TraversalVertexProgram` provides a fluent `Builder` for constructing a `TraversalVertexProgram`. The specified `traversal()` can be either a `Supplier<Traversal>` object, a `Supplier<Traversal>` class, or a link:http://en.wikipedia.org/wiki/Scripting_for_the_Java_Platform[JSR-223] script that will generate (i.e. supply) a `Traversal`. If `traversal()` is supplied a single string, it is assumed that "gremlin-groovy" is the `ScriptEngine` to use. If two strings are supplied, then the first string denotes the `ScriptEngine` to evaluate the second string script with in order to generate (i.e. supply) the `Traversal`.

[source,groovy]
gremlin> result = g.compute().program(TraversalVertexProgram.build().traversal("com.tinkerpop.gremlin.tinkergraph.structure.TinkerGraph.open().V().both().has('age').values('age').groupCount('a')").create()).submit().get()
==>result[tinkergraph[vertices:6 edges:6],memory[size:3]]
gremlin> result.memory().a
==>32=3
==>35=1
==>27=1
==>29=3
gremlin> result.memory().iteration
==>5
gremlin> result.memory().runtime
==>7

[[distributed-gremlin-gotchas]]
Distributed Gremlin Gotchas
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Gremlin OLTP is not identical to Gremlin OLAP.

IMPORTANT: There are two primary theoretical differences between Gremlin OLTP and Gremlin OLAP. First, Gremlin OLTP (via `Traversal`) leverages a link:http://en.wikipedia.org/wiki/Depth-first_search[depth-first] execution engine. Depth-first execution has a limited memory footprint due to link:http://en.wikipedia.org/wiki/Lazy_evaluation[lazy evaluation]. On the other hand, Gremlin OLAP (via `TraversalVertexProgram`) leverages a link:http://en.wikipedia.org/wiki/Breadth-first_search[breadth-first] execution engine which maintains a larger memory footprint, but a better time complexity due to vertex-local traversers being able to be merged (via "bulking"). The second difference is that Gremlin OLTP is executed in a serial fashion, while Gremlin OLAP is executed in a parallel fashion. These two fundamental differences lead to the behaviors enumerated below.

image::gremlin-without-a-cause.png[width=200,float=right]

 . Traversal sideEffects are represented as a distributed data structure across the graph's vertex set. It is not possible to get a global view of a sideEffect until it is aggregated via a <<mapreduce,MapReduce>> job. In some situations, the local vertex representation of the sideEffect is sufficient to ensure the intended semantics of the traversal are respected. However, this is not generally true so be wary of traversals that require global views of a sideEffect.
 . When evaluating traversals that rely on path information (i.e. the history of the traversal), practical computational limits can easily be reached due the link:http://en.wikipedia.org/wiki/Combinatorial_explosion[combinatoric explosion] of data. With path computing enabled, every traverser is unique and thus, must be enumerated as opposed to being bulked. The difference being a collection of paths vs. a single 64-bit long at a single vertex. For more information on this concept, please see link:http://thinkaurelius.com/2012/11/11/faunus-provides-big-graph-data-analytics/[Faunus Provides Big Graph Data].
 . When traversals of the form `x.as('a').y.someSideEffectStep('a').z` are evaluated, the `a` sideEffect is stored in the path information of the traverser and thus, such traversals turn on path calculations when executed on a GraphComputer.
 . Steps that are concerned with the global ordering of traversers do not have a meaningful representation in OLAP. For example, what does <<order-step,`order()`>>-step mean when all traversers are being processed in parallel? Even if the traversers were aggregated and ordered, then at the next step they would return to being executed in parallel and thus, in an unpredictable order. Other steps of this nature include <<orderby-step,`orderby()`>> and <<shuffle-step,`shuffle()`>>. When these steps are executed at the end of a traversal (i.e the final step), the `TraverserMapReduce` job ensures the resultant serial representation is ordered accordingly.
 . Steps that are concerned with providing a global aggregate to the next step of computation do not have a correlate in OLAP. For example, <<fold-step,`fold()`>>-step can only fold up the objects at each executing vertex. Next, even if a global fold was possible, where would it go? Which vertex would be the host of the data structure? The `fold()`-step only makes sense as an end-step whereby a MapReduce job can generate the proper global-to-local data reduction.